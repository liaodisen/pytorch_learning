{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Pytorch Fashion of building a model:\n",
    "1. Prepare Dataset\n",
    " - Covered later\n",
    "2. Design model using Class\n",
    " - Inherit from nn.Module\n",
    "3. Construct loss and optimizer\n",
    " - Using PyTorch API\n",
    "4. Training Cycle\n",
    " - forward, backward, update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "the model is simple linear regression\n",
    "$\\hat{y} = wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design model\n",
    "\n",
    "`torch.nn.Linear(in_feature, out_feature, bias=True)`\n",
    "Applies a linear transformation to the incoming data: $y = Ax + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design the model\n",
    "class LineaModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LineaModel, self).__init__()\n",
    "        ## Create an object\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## This is the callable instance\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "## Create a instance of class\n",
    "model = LineaModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callable function example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello1\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Foobar:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        print(\"Hello\" + str(args[0]))\n",
    "\n",
    "foobar = Foobar()\n",
    "foobar(1, 2, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construct Loss and Optimizaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liaodisen/opt/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Cycle\n",
    "1) get $\\hat{y}$\n",
    "2) get loss\n",
    "3) backward\n",
    "4) update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(95.0057, grad_fn=<MseLossBackward0>)\n",
      "1 tensor(42.9586, grad_fn=<MseLossBackward0>)\n",
      "2 tensor(19.7791, grad_fn=<MseLossBackward0>)\n",
      "3 tensor(9.4508, grad_fn=<MseLossBackward0>)\n",
      "4 tensor(4.8437, grad_fn=<MseLossBackward0>)\n",
      "5 tensor(2.7836, grad_fn=<MseLossBackward0>)\n",
      "6 tensor(1.8575, grad_fn=<MseLossBackward0>)\n",
      "7 tensor(1.4363, grad_fn=<MseLossBackward0>)\n",
      "8 tensor(1.2401, grad_fn=<MseLossBackward0>)\n",
      "9 tensor(1.1441, grad_fn=<MseLossBackward0>)\n",
      "10 tensor(1.0928, grad_fn=<MseLossBackward0>)\n",
      "11 tensor(1.0616, grad_fn=<MseLossBackward0>)\n",
      "12 tensor(1.0395, grad_fn=<MseLossBackward0>)\n",
      "13 tensor(1.0215, grad_fn=<MseLossBackward0>)\n",
      "14 tensor(1.0054, grad_fn=<MseLossBackward0>)\n",
      "15 tensor(0.9904, grad_fn=<MseLossBackward0>)\n",
      "16 tensor(0.9759, grad_fn=<MseLossBackward0>)\n",
      "17 tensor(0.9617, grad_fn=<MseLossBackward0>)\n",
      "18 tensor(0.9478, grad_fn=<MseLossBackward0>)\n",
      "19 tensor(0.9342, grad_fn=<MseLossBackward0>)\n",
      "20 tensor(0.9207, grad_fn=<MseLossBackward0>)\n",
      "21 tensor(0.9075, grad_fn=<MseLossBackward0>)\n",
      "22 tensor(0.8945, grad_fn=<MseLossBackward0>)\n",
      "23 tensor(0.8816, grad_fn=<MseLossBackward0>)\n",
      "24 tensor(0.8689, grad_fn=<MseLossBackward0>)\n",
      "25 tensor(0.8565, grad_fn=<MseLossBackward0>)\n",
      "26 tensor(0.8441, grad_fn=<MseLossBackward0>)\n",
      "27 tensor(0.8320, grad_fn=<MseLossBackward0>)\n",
      "28 tensor(0.8201, grad_fn=<MseLossBackward0>)\n",
      "29 tensor(0.8083, grad_fn=<MseLossBackward0>)\n",
      "30 tensor(0.7967, grad_fn=<MseLossBackward0>)\n",
      "31 tensor(0.7852, grad_fn=<MseLossBackward0>)\n",
      "32 tensor(0.7739, grad_fn=<MseLossBackward0>)\n",
      "33 tensor(0.7628, grad_fn=<MseLossBackward0>)\n",
      "34 tensor(0.7518, grad_fn=<MseLossBackward0>)\n",
      "35 tensor(0.7410, grad_fn=<MseLossBackward0>)\n",
      "36 tensor(0.7304, grad_fn=<MseLossBackward0>)\n",
      "37 tensor(0.7199, grad_fn=<MseLossBackward0>)\n",
      "38 tensor(0.7095, grad_fn=<MseLossBackward0>)\n",
      "39 tensor(0.6993, grad_fn=<MseLossBackward0>)\n",
      "40 tensor(0.6893, grad_fn=<MseLossBackward0>)\n",
      "41 tensor(0.6794, grad_fn=<MseLossBackward0>)\n",
      "42 tensor(0.6696, grad_fn=<MseLossBackward0>)\n",
      "43 tensor(0.6600, grad_fn=<MseLossBackward0>)\n",
      "44 tensor(0.6505, grad_fn=<MseLossBackward0>)\n",
      "45 tensor(0.6412, grad_fn=<MseLossBackward0>)\n",
      "46 tensor(0.6319, grad_fn=<MseLossBackward0>)\n",
      "47 tensor(0.6229, grad_fn=<MseLossBackward0>)\n",
      "48 tensor(0.6139, grad_fn=<MseLossBackward0>)\n",
      "49 tensor(0.6051, grad_fn=<MseLossBackward0>)\n",
      "50 tensor(0.5964, grad_fn=<MseLossBackward0>)\n",
      "51 tensor(0.5878, grad_fn=<MseLossBackward0>)\n",
      "52 tensor(0.5794, grad_fn=<MseLossBackward0>)\n",
      "53 tensor(0.5710, grad_fn=<MseLossBackward0>)\n",
      "54 tensor(0.5628, grad_fn=<MseLossBackward0>)\n",
      "55 tensor(0.5548, grad_fn=<MseLossBackward0>)\n",
      "56 tensor(0.5468, grad_fn=<MseLossBackward0>)\n",
      "57 tensor(0.5389, grad_fn=<MseLossBackward0>)\n",
      "58 tensor(0.5312, grad_fn=<MseLossBackward0>)\n",
      "59 tensor(0.5235, grad_fn=<MseLossBackward0>)\n",
      "60 tensor(0.5160, grad_fn=<MseLossBackward0>)\n",
      "61 tensor(0.5086, grad_fn=<MseLossBackward0>)\n",
      "62 tensor(0.5013, grad_fn=<MseLossBackward0>)\n",
      "63 tensor(0.4941, grad_fn=<MseLossBackward0>)\n",
      "64 tensor(0.4870, grad_fn=<MseLossBackward0>)\n",
      "65 tensor(0.4800, grad_fn=<MseLossBackward0>)\n",
      "66 tensor(0.4731, grad_fn=<MseLossBackward0>)\n",
      "67 tensor(0.4663, grad_fn=<MseLossBackward0>)\n",
      "68 tensor(0.4596, grad_fn=<MseLossBackward0>)\n",
      "69 tensor(0.4530, grad_fn=<MseLossBackward0>)\n",
      "70 tensor(0.4465, grad_fn=<MseLossBackward0>)\n",
      "71 tensor(0.4401, grad_fn=<MseLossBackward0>)\n",
      "72 tensor(0.4337, grad_fn=<MseLossBackward0>)\n",
      "73 tensor(0.4275, grad_fn=<MseLossBackward0>)\n",
      "74 tensor(0.4214, grad_fn=<MseLossBackward0>)\n",
      "75 tensor(0.4153, grad_fn=<MseLossBackward0>)\n",
      "76 tensor(0.4093, grad_fn=<MseLossBackward0>)\n",
      "77 tensor(0.4035, grad_fn=<MseLossBackward0>)\n",
      "78 tensor(0.3977, grad_fn=<MseLossBackward0>)\n",
      "79 tensor(0.3919, grad_fn=<MseLossBackward0>)\n",
      "80 tensor(0.3863, grad_fn=<MseLossBackward0>)\n",
      "81 tensor(0.3808, grad_fn=<MseLossBackward0>)\n",
      "82 tensor(0.3753, grad_fn=<MseLossBackward0>)\n",
      "83 tensor(0.3699, grad_fn=<MseLossBackward0>)\n",
      "84 tensor(0.3646, grad_fn=<MseLossBackward0>)\n",
      "85 tensor(0.3593, grad_fn=<MseLossBackward0>)\n",
      "86 tensor(0.3542, grad_fn=<MseLossBackward0>)\n",
      "87 tensor(0.3491, grad_fn=<MseLossBackward0>)\n",
      "88 tensor(0.3441, grad_fn=<MseLossBackward0>)\n",
      "89 tensor(0.3391, grad_fn=<MseLossBackward0>)\n",
      "90 tensor(0.3342, grad_fn=<MseLossBackward0>)\n",
      "91 tensor(0.3294, grad_fn=<MseLossBackward0>)\n",
      "92 tensor(0.3247, grad_fn=<MseLossBackward0>)\n",
      "93 tensor(0.3200, grad_fn=<MseLossBackward0>)\n",
      "94 tensor(0.3154, grad_fn=<MseLossBackward0>)\n",
      "95 tensor(0.3109, grad_fn=<MseLossBackward0>)\n",
      "96 tensor(0.3064, grad_fn=<MseLossBackward0>)\n",
      "97 tensor(0.3020, grad_fn=<MseLossBackward0>)\n",
      "98 tensor(0.2977, grad_fn=<MseLossBackward0>)\n",
      "99 tensor(0.2934, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss)\n",
    "\n",
    "    # the grad computed  by .backward() will be accumulated.\n",
    "    # So before backward, remember set the grad to zero\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  1.6393945217132568\n",
      "b =  0.8197411298751831\n",
      "y_pred =  tensor([[7.3773]])\n"
     ]
    }
   ],
   "source": [
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())\n",
    "\n",
    "x_test = torch.Tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "166fc0a0cd29562202c5074696ab677143cae73c65badbaa95ef6455ab21a751"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
